# Machine-Learning-Project-1
Predicting the Poisonous Mushroom

## Numpy:
* NumPy offers comprehensive mathematical functions, random number generators, linear algebra routines, Fourier transforms, and more.
* NumPy supports a wide range of hardware and computing platforms and plays well with distributed, GPU, and sparse array libraries
* NumPy’s high-level syntax makes it accessible and productive for programmers from any background or experience level.

## Pandas:
* Pandas is a fast, powerful, flexible, and easy-to-use open-source data analysis and manipulation tool, built on top of the Python programming language.

## .iloc[]:
* .iloc[] is primarily integer position-based (from 0 to length-1 of the axis), but may also be used with a boolean array.

## .isnull().sum:
*  The function dataframe.isnull().sum().sum() returns the number of missing values in the dataset.

## Scikit-learn:
The sklearn library contains a lot of efficient tools for machine learning and statistical modeling including classification, regression, clustering, and dimensionality reduction.
* Simple and efficient tools for predictive data analysis
* Accessible to everybody, and reusable in various contexts
* Built on NumPy, SciPy, and matplotlib
* Open source, commercially usable - BSD license

## sklearn.model_selection.train_test_split:
* Split arrays or matrices into random train and test subsets.
* sklearn.model_selection.train_test_split(*arrays, test_size=None, random_state=None)
  * <strong>*arrays:</strong> Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes.
  *  **test_size - float or int, default=None:** If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. If train_size is also None, it will be set to 0.25.
  *  **random_state - int, RandomState instance or None, default=None:** random_state is basically used for reproducing your problem the same every time it is run. If you do not use a random_state in train_test_split, every time you make the split you might get a different set of train and test data points and will not help you in debugging in case you get an issue.

## Feature Scaling:
Feature Scaling is a technique to standardize the independent features present in the data in a fixed range. It is performed during the data pre-processing to handle highly varying magnitudes or values or units. If feature scaling is not done, then a machine learning algorithm tends to weigh greater values, higher and consider smaller values as the lower values, regardless of the unit of the values.
```
    from sklearn.preprocessing import StandardScaler
    sc = StandardScaler()
    X_train = sc.fit_transform(X_train)
    X_test = sc.transform(X_test)
```

## Logistic-Regression:
Logistic regression is a supervised machine learning algorithm used for classification tasks where the goal is to predict the probability that an instance belongs to a given class or not. Logistic regression is a statistical algorithm which analyze the relationship between two data factors. The article explores the fundamentals of logistic regression, it’s types and implementations.
 ```
    from sklearn.linear_model import LogisticRegression
    classifier = LogisticRegression(random_state = 42)
    classifier.fit(X_train, y_train)
```

## classifier.predict():
classifier.predict() is a method used in machine learning to generate predictions based on a trained model.

## Confusion Matrix : 
The confusion matrix, generated by the “confusion_matrix()” function from “sklearn.metrics”, is a table that summarizes the performance of a classification algorithm. It displays the counts of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions. The matrix is typically represented in a 2x2 format for binary classification tasks and extends to higher dimensions for multi-class classification
* **Accuracy Score :** The accuracy score, computed using the “accuracy_score()” function from “sklearn.metrics”, quantifies the overall accuracy of the classifier. It calculates the proportion of correctly classified instances out of the total number of instances in the dataset. The accuracy score is a commonly used evaluation metric for classification models, providing a simple and intuitive measure of performance.
* **Precision Score:**  The accuracy score, computed using accuracy_score() from sklearn.metrics, quantifies the overall correctness of the model's predictions. It represents the proportion of correctly classified instances out of the total number of instances in the dataset.
*  **Recall Score:**  The recall score, determined by recall_score() from sklearn.metrics, evaluates the ratio of true positive predictions to the total number of actual positive instances in the dataset. It reflects the model's sensitivity in capturing positive instances.

## K-Nearest Neighbors : 
K-Nearest Neighbors (KNN) is a machine learning algorithm that makes predictions based on the similarity of data points in a feature space. It classifies or regresses new instances by considering the majority class or average value among its k nearest neighbors, respectively. KNN is non-parametric, lazy learning algorithm, suitable for various tasks like classification, regression, and clustering, particularly effective with datasets of small dimensionality.
```
    from sklearn.neighbors import KNeighborsClassifier
    classifier = KNeighborsClassifier(n_neighbors = 5, metric 
    'minkowski', p = 2)
    classifier.fit(X_train, y_train)
```

* **n_neighbors:**  Specifies the number of neighbors to consider when making predictions. In this case, n_neighbors = 5 indicates that the classifier will consider the 5 nearest neighbors to the query point.
* **metric:** Determines the distance metric used to measure the similarity between instances. The 'minkowski' metric is selected, indicating that the Minkowski distance will be used. The specific distance metric can be further customized based on the problem requirements.
* **p:** Parameter for the Minkowski distance calculation. When p = 2, the Euclidean distance is used.
* The **fit()** method adjusts the internal parameters of the KNN model to minimize the prediction error on the training data, effectively learning the relationships between the input features and the target labels.

## Comparison of KNN and LogisticRegression:
* **KNN :**
    *  Accuracy = 0.9891923902583463
    *  precision score = 0.9878591663294213
    *  Recall score = 0.9924108957853368
* **LogisticRegression :**
    * accuracy score = 0.6344659116144792
    * precision score = 0.6496994971176254
    * Recall score = 0.7178479468762705
